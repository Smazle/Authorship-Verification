\section{Method}

\subsection{Text Features}
Feature extraction from a text is the process of finding a vector that
represents the text. There are a lot of different features that can be extracted
and we use several of them. Specifically we use some character level features,
some word level features and som \gls{POS} tagging features. In this section
we will define the different features we use. The specific features used in
different experiments will be described under those experiments.

N-grams are subsequences extracted from a sequence of tokens. For example
3-grams is all subsequences of length three of a given sequence. For example
using individual characters as tokens all \textit{character 3-grams} of the
string "hello" is "hel", "ell" and "llo". We use several different types of
n-grams in different experiments including character n-grams, word n-grams,
special character n-grams and \gls{POS} tagging n-grams. Word n-grams are
subsequences of words, special character n-grams are subsequences of characters
with alphanumeric characters removed, \gls{POS} tagging n-grams are subsequences
of \gls{POS} tags. A special case of n-grams is 1-grams which is just a count of
the different tokens in a sequence. We will refer to 1-grams as frequencies.

\subsection{Delta Method}
We have chosen to use the Delta method as a baseline method for our other
implementations. The Delta method is described by \cite{evert2015towards} and
consist of extracting word frequencies, applying a linear transformation to
those frequencies and using \gls{KNN} with different distance metrics. There are
a number of parameters to choose. First of all the number of different words to
find frequencies for has to be chosen (it was originally chosen at 150 words
\cite{evert2015towards}). Then the linear transformation can be chosen, the
usual transformation is a normalization to zero mean unit variance. And finally
the distance metric can be chosen.

\subsection{Generalising Random Forest}
In addition to the delta method we chose to use the Random Forest approach
suggested by \cite{pacheco2015}. In our implementation we use different
features than in the their proposal. The idea of the method is to learn a
general difference between feature vectors of texts written by the same author
and feature vectors written by the different authors. The generalization is
obtained by combining the vectors of known texts with vectors of unknown
text and training on that combination instead of on the raw vectors. In
\cite{pacheco2015} the combining function is,

\begin{equation}
    \left\langle
        \dfrac{(A_i-U_i)^2+1}{(B-U_i)^2+1}|i \in [0,\dots, N)
    \right\rangle,
\end{equation}

where $A_i$ is the $i$'th author-specific feature vector, $B$ is a \gls{UBM}
describing a general author independent text and $U_i$ is the feature vector of
the $i$'th unknown text.

The \gls{UBM} is meant to represent the features of an author independent text.
It is computed by concatenating all texts in the training dataset and computing
features from that resulting text. Since multiple authors is then part of the
text we are computing features from, the assumption is that the author specific
features will be averaged out and the \gls{UBM} will represent the features of
an author independent text. The addition of 1 in the model prevent division
by zero. Since 1 is added both in the numerator and the denominator it does
not otherwise change the result. The squaring of $(A_i - U_i)$ and $(B - U_i)$
prevents negative values. Therefore each value in the resulting encoded feature
vector is in the range $[0; \inf[$.

Let's examine what the above equation describes. Fix any specific $i$ and let
$A$ be $A_i$ and $U$ be $U_i$ then for each feature $k$ we compute,

\begin{equation}
    R_k = \frac{(A_k-U_k)^2+1}{(B_k-U_k)^2+1},
\end{equation}

The $k$'th feature in each of the vectors is the same feature just extracted
from different texts. When the feature of the unknown text $U_k$ is closer to
$A_k$ than $B_k$ the numerator in the fraction will be greater than the
denominator giving us something in the range $[0; 1]$. When the feature of the
unknown text $U_k$ is closer to $B_k$ than $A_k$ the numerator will be lesser
than the denominator and we will therefore get a value in the interval
$[1; \infty[$. That results in $R$ being a vector containing values from 0 to
$\infty$ where it is between 0 and 1 whenever a feature is closer to the author
specific text than the universal text and greater than 1 otherwise.

The random forest algorithm is then trained on these encoded feature vectors
where it is supposed to learn a general difference between feature vectors of
the same authors and feature vectors of different authors.

\subsection{Extended Delta}
% TODO write.

\subsection{Author Specific SVM}
We implemented an approach using \gls{SVM}'s inspired by \cite{hansen2014}. The
approach is only applicable to problems with more than a single text per
unknown author. The classification in this approach is done by training an \gls{SVM}
classifier on all known texts of an author and an equal number of texts from
other authors. Then the unknown text is given to the \gls{SVM} and is classified
either as belonging to the same author or as belonging to a different author.

If there is only a single known text available for an author it does not make
sense to train an \gls{SVM} since there is simply to little data.

\subsection{Author Specific Random Forest}
% TODO: write.

\subsection{Experiments}
In this section we describe the different experiments we have performed.
We have tested the different methods we have implemented with different
features and on different datasets. In our experiments we use data from PAN
2013 \footnote{http://pan.webis.de/clef13/pan13-web/index.html} and PAN 2015
\footnote{http://pan.webis.de/clef15/pan15-web/index.html}. The PAN 2013 data
consist of texts from English, Greek and Spanish authors. We work only on the
English texts which consist of texts from 10 authors. There is a number (between
1 and 10) known texts from each author and a single unknown text. The task is
given the known texts of an author to determine whether or not the unknown text
is written by the same author or not.

The PAN 2015 data consist of texts by authors in English, Dutch, Greek and
Spanish. Again we only work with English texts. Unlike the 2013 dataset there is
only a single known text for each author and a single unknown text. There is
therefore much less known data available for each author which makes the
verification harder. In the 2015 dataset there is 100 \textit{problems} in which
some of the known texts are the same (i.e. there is not 100 authors).

% TODO: Describe brown.

% TODO Change delta to brown.
\subsubsection{Delta Method}
In the delta method we work only with word frequencies as originally proposed.
As the linear function we normalize to 0 mean and unit variance and as features
we use the $n$ most frequent words. We extract the most frequent words by
counting all words in the text choosing the $n$ most frequent and dividing the
counts by the total number of words. In the \gls{KNN} part we use the Manhatten
distance and only a single nearest neighbour since in one of the datasets we
have only 1 text for each author. To classify the unknown texts as either
written by or not written by the author we train a \gls{KNN} for each unknown
text.  Each classifier is trained with a known text from the author in question
and $k$ other random texts. If the unknown text is classified as belonging
to one of the $k$ random authors instead of the author in question we report
that the unknown text is not written by the author. If the text is classified
as belonging to the author in question we classify it as being written by the
author.

We chose the number of most frequent words $n$ and number of opposing
authors $k$ by trying different configurations in a grid and choosing the
best values. On the training dataset we obtained the results shown in
Figure \ref{fig:delta_pan_2013_res} for the PAN 2013 data and in Figure
\ref{fig:delta_pan_2015_res} for the PAN 2015 data. For PAN 2013 the results are
generally better since there is more text available and the best accuracy were
obtained when using the 300 most frequent words and 5 opposing authors. For PAN
2015 the best result is obtained when using the 200 most frequent words and 2
opposing authors.

\begin{figure}
    \centering
    \begin{tabular}{c|ccccc}
               & $n=100$ & $n=150$ & $n=200$ & $n=250$ & $n=300$ \\
        \hline
        $k=1$  & 0.63320 & 0.63791 & 0.65370 & 0.63837 & 0.65740 \\
        $k=2$  & 0.68308 & 0.68670 & 0.67825 & 0.65062 & 0.66201 \\
        $k=3$  & 0.66750 & 0.70228 & 0.69271 & 0.66620 & 0.69872 \\
        $k=4$  & 0.71116 & 0.67245 & 0.64852 & 0.67541 & 0.69525 \\
        $k=5$  & 0.70545 & 0.69491 & 0.65249 & 0.68393 & 0.76134 \\
        $k=6$  & 0.65678 & 0.65666 & 0.64437 & 0.66838 & 0.67423 \\
        $k=7$  & 0.67728 & 0.70379 & 0.66705 & 0.68670 & 0.73045 \\
        $k=8$  & 0.72870 & 0.69072 & 0.70008 & 0.66928 & 0.73800 \\
        $k=9$  & 0.72112 & 0.71179 & 0.67058 & 0.73857 & 0.73045 \\
        $k=10$ & 0.70370 & 0.69513 & 0.66947 & 0.70593 & 0.71544
    \end{tabular}
    \caption{Accuracy on different number of most frequent words and different
        number of opposing authors for the Delta Method. Each number is a
        average of 10 runs since there is randomness involved when picking the
        opposing authors. The test is run on the 2013 dataset.}
    \label{fig:delta_pan_2013_res}
\end{figure}

\begin{figure}
    \centering
    \begin{tabular}{c|ccccc}
               & $n=100$ & $n=150$ & $n=200$ & $n=250$ & $n=300$ \\
        \hline
        $k=1$  & 0.51800 & 0.55899 & 0.55800 & 0.55199 & 0.57300 \\
        $k=2$  & 0.53000 & 0.54899 & 0.59800 & 0.57400 & 0.58600 \\
        $k=3$  & 0.52300 & 0.57000 & 0.57700 & 0.58700 & 0.56900 \\
        $k=4$  & 0.51400 & 0.56600 & 0.55700 & 0.56200 & 0.55500 \\
        $k=5$  & 0.53300 & 0.54300 & 0.55400 & 0.55999 & 0.56199 \\
        $k=6$  & 0.51300 & 0.55699 & 0.55000 & 0.55199 & 0.55700 \\
        $k=7$  & 0.50600 & 0.55400 & 0.52100 & 0.53200 & 0.55500 \\
        $k=8$  & 0.49400 & 0.53000 & 0.54000 & 0.54300 & 0.53800 \\
        $k=9$  & 0.50300 & 0.53600 & 0.54000 & 0.53600 & 0.53999 \\
        $k=10$ & 0.49600 & 0.52900 & 0.53600 & 0.54200 & 0.54399
    \end{tabular}
    \caption{Accuracy on different number of most frequent words and different
        number of opposing authors for the Delta Method. Each number is a
        average of 10 runs since there is randomness involved when picking the
        opposing authors. The test is run on the 2015 dataset.}
    \label{fig:delta_pan_2015_res}
\end{figure}

% TODO: Describe number of trees and other parameters.
% TODO: Tables of tests.
\subsubsection{Generalising Random Forest}
TBA


In the test of our random forest implementation we used a combination
of different features. We used the 20 most frequent character
2-grams, the 20 most frequent 3-grams, the 10 most frequent
word 3-grams and the 10 most frequent \gls{POS} tag 3-grams.
We used the random forest implementation from \texttt{sklearn}
\footnote{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}.

We tested our solution by splitting the training data into two sets a set used
for training and a set used for testing. For this test we split with 80\% being
the training set, and the rest being the test set. We computed the average of
100 tests to account for randomness. The accuracy on the 2015 dataset resulted
only in 0.48 accuracy.

\subsection{Extended Delta}
% TODO write.

\subsection{Author Specific SVM} \label{subsec:author_specific_svm}
In this approach we work with a mixture of different features. We use the 500
most frequent 3-, 4- and 5-character-grams, the 100 most frequent 3- an
4-word-grams, the 20 most frequent 2-, 3- and 4-postag-grams. The most frequent
grams is found in the brown text corpus. We run only on the PAN 2013 dataset
since 2015 contains only a single known text per author and it would therefore
not make sense to use this method. We use the \textit{sklearn} implementation of
\gls{SVM}'s which internally use \textit{libsvm}.

For each author $a$ in the 2013 dataset we compute features for all known texts
$k$ and give them the class 1. Then we choose the $|k|$ number of random texts
written by other authors and give them the class 0. We then train an \gls{SVM}
classifier on the features and classes generated and use it to predict either 0
or 1 for the unknown text. The \gls{SVM} use a rbf kernel with $C=1$ and
$\gamma = frac{1}{(2|k|)}$. Using that approach we get an average accuracy over
100 runs of 0.736.

\subsection{Author Specific Random Forest}
In this approach we use the same features as in
\ref{subsec:author_specific_svm}. The only difference is that we train a random
forest instead of an \gls{SVM} on the data. The forest has 7 decision trees and
we use the \textit{sklearn} implementation. Using that approach we get an
accurate accuracy over 100 runs of TODO.
