\section{Method}

\subsection{Text Features}
Feature extraction from a text is the process of finding a vector that
represents the text. There are a lot of different features that can be extracted
and we use several of them. Specifically we use some character level features,
some word level features and som \gls{POS} tagging features. In this section
we will define the different features we use. The specific features used in
different experiments will be described under those experiments.

N-grams are subsequences extracted from a sequence of tokens. For example
3-grams is all subsequences of length three of a given sequence. For example
using individual characters as tokens all \textit{character 3-grams} of the
string "hello" is "hel", "ell" and "llo". We use several different types of
n-grams in different experiments including character n-grams, word n-grams,
special character n-grams and \gls{POS} tagging n-grams. Word n-grams are
subsequences of words, special character n-grams are subsequences of characters
with alphanumeric characters removed, \gls{POS} tagging n-grams are subsequences
of \gls{POS} tags. A special case of n-grams is 1-grams which is just a count of
the different tokens in a sequence. We will refer to 1-grams as frequencies.

\subsection{Delta Method}
We have chosen to use the Delta method as a baseline method for our other
implementations. The Delta method is described by \cite{evert2015towards} and
consist of extracting word frequencies, applying a linear transformation to
those frequencies and using \gls{KNN} with different distance metrics. There are
a number of parameters to choose. First of all the number of different words to
find frequencies for has to be chosen (it was originally chosen at 150 words
\cite{evert2015towards}). Then the linear transformation can be chosen, the
usual transformation is a normalization to zero mean unit variance. And finally
the distance metric can be chosen.

\subsection{Random Forest}
In addition to the delta method, we chose to use the Random Forest Algorithm.
Normally one would train out forest on each of the authors individually, then
use that model to verify whether or not the proposed author was is the same
one.
However due to the small amount of that associated with each author in the PAN
2015 data, this approach wasn't feasible, as it lacks generalization
capabilties. \cite{pacheco2015} offers a way to achieve this generalization
using \gls{UBM}. A model describing the general text characteristics of the
entire data set is created, this serves as our \gls{UBM}. Using our \gls{UBM},
and a author-specific text, we can compute a match score for an introduced
author-less text by comparing it to the \gls{UBM}, and the author-specific test.

The match score in our case is computed as follows:
\begin{equation}
    \left\langle
        \dfrac{(A_i-U_i)^2+1}{(B-U_i)^2+1}|i \in [0,\dots, N)
    \right\rangle,
\end{equation}
Where $A_i$ is the i'th author-specific feature vector. $B$ is the \gls{UBM}
computed by taking concatenating out set of author-specific texts, and then
performing the same set of feature extractions on that set of texts, as we do on
each individual author-specific text, and our unknown text. Finally there $U_i$
which is the i'th unknown author text feature vector.

The addition of of 1 serves, to prevent division by zero, and squaring by two,
prevent negative values. As such, each value in the resulting encoded feature
vector is in the range $[0,\dots, 1)$ if it's closer to the \gls{UBM} in terms
of value, and in range $[0,\dots, \inf^+)$ if it's closer to the author-specific
text. Thus describing the three way relation between the unknown text, the
author-specific text, and the \gls{UBM}.

It's with this generalized encoded feature matrix, that we train our random
forest.

\subsection{Experiments}
In this section we describe the different experiments we have performed.
We have tested the different methods we have implemented with different
features and on different datasets. In our experiments we use data from PAN
2013 \footnote{http://pan.webis.de/clef13/pan13-web/index.html} and PAN 2015
\footnote{http://pan.webis.de/clef15/pan15-web/index.html}. The PAN 2013 data
consist of texts from English, Greek and Spanish authors. We work only on the
English texts which consist of texts from 10 authors. There is a number (between
1 and 10) known texts from each author and a single unknown text. The task is
given the known texts of an author to determine whether or not the unknown text
is written by the same author or not.

The PAN 2015 data consist of texts by authors in English, Dutch, Greek and
Spanish. Again we only work with English texts. Unlike the 2013 dataset there is
only a single known text for each author and a single unknown text. There is
therefore much less known data available for each author which makes the
verification harder. In the 2015 dataset there is 100 \textit{problems} in which
some of the known texts are the same (i.e. there is not 100 authors).

\subsubsection{Delta Method}
In the delta method we work only with word frequencies as originally proposed.
As the linear function we normalize to 0 mean and unit variance and as features
we use the $n$ most frequent words. We extract the most frequent words by
counting all words in the text choosing the $n$ most frequent and dividing the
counts by the total number of words. In the \gls{KNN} part we use the Manhatten
distance and only a single nearest neighbour since in one of the datasets we
have only 1 text for each author. To classify the unknown texts as either
written by or not written by the author we train a \gls{KNN} for each unknown
text.  Each classifier is trained with a known text from the author in question
and $k$ other random texts. If the unknown text is classified as belonging
to one of the $k$ random authors instead of the author in question we report
that the unknown text is not written by the author. If the text is classified
as belonging to the author in question we classify it as being written by the
author.

We chose the number of most frequent words $n$ and number of opposing
authors $k$ by trying different configurations in a grid and choosing the
best values. On the training dataset we obtained the results shown in
Figure \ref{fig:delta_pan_2013_res} for the PAN 2013 data and in Figure
\ref{fig:delta_pan_2015_res} for the PAN 2015 data. For PAN 2013 the results are
generally better since there is more text available and the best accuracy were
obtained when using the 300 most frequent words and 5 opposing authors. For PAN
2015 the best result is obtained when using the 200 most frequent words and 2
opposing authors.

\begin{figure}
    \centering
    \begin{tabular}{c|ccccc}
               & $n=100$ & $n=150$ & $n=200$ & $n=250$ & $n=300$ \\
        \hline
        $k=1$  & 0.63320 & 0.63791 & 0.65370 & 0.63837 & 0.65740 \\
        $k=2$  & 0.68308 & 0.68670 & 0.67825 & 0.65062 & 0.66201 \\
        $k=3$  & 0.66750 & 0.70228 & 0.69271 & 0.66620 & 0.69872 \\
        $k=4$  & 0.71116 & 0.67245 & 0.64852 & 0.67541 & 0.69525 \\
        $k=5$  & 0.70545 & 0.69491 & 0.65249 & 0.68393 & 0.76134 \\
        $k=6$  & 0.65678 & 0.65666 & 0.64437 & 0.66838 & 0.67423 \\
        $k=7$  & 0.67728 & 0.70379 & 0.66705 & 0.68670 & 0.73045 \\
        $k=8$  & 0.72870 & 0.69072 & 0.70008 & 0.66928 & 0.73800 \\
        $k=9$  & 0.72112 & 0.71179 & 0.67058 & 0.73857 & 0.73045 \\
        $k=10$ & 0.70370 & 0.69513 & 0.66947 & 0.70593 & 0.71544
    \end{tabular}
    \caption{Accuracy on different number of most frequent words and different
        number of opposing authors for the Delta Method. Each number is a
        average of 10 runs since there is randomness involved when picking the
        opposing authors. The test is run on the 2013 dataset.}
    \label{fig:delta_pan_2013_res}
\end{figure}

\begin{figure}
    \centering
    \begin{tabular}{c|ccccc}
               & $n=100$ & $n=150$ & $n=200$ & $n=250$ & $n=300$ \\
        \hline
        $k=1$  & 0.51800 & 0.55899 & 0.55800 & 0.55199 & 0.57300 \\
        $k=2$  & 0.53000 & 0.54899 & 0.59800 & 0.57400 & 0.58600 \\
        $k=3$  & 0.52300 & 0.57000 & 0.57700 & 0.58700 & 0.56900 \\
        $k=4$  & 0.51400 & 0.56600 & 0.55700 & 0.56200 & 0.55500 \\
        $k=5$  & 0.53300 & 0.54300 & 0.55400 & 0.55999 & 0.56199 \\
        $k=6$  & 0.51300 & 0.55699 & 0.55000 & 0.55199 & 0.55700 \\
        $k=7$  & 0.50600 & 0.55400 & 0.52100 & 0.53200 & 0.55500 \\
        $k=8$  & 0.49400 & 0.53000 & 0.54000 & 0.54300 & 0.53800 \\
        $k=9$  & 0.50300 & 0.53600 & 0.54000 & 0.53600 & 0.53999 \\
        $k=10$ & 0.49600 & 0.52900 & 0.53600 & 0.54200 & 0.54399
    \end{tabular}
    \caption{Accuracy on different number of most frequent words and different
        number of opposing authors for the Delta Method. Each number is a
        average of 10 runs since there is randomness involved when picking the
        opposing authors. The test is run on the 2015 dataset.}
    \label{fig:delta_pan_2015_res}
\end{figure}

% TODO: Describe number of trees and other parameters.
% TODO: Tables of tests.
\subsubsection{Random Forest}
TBA


In the test of our random forest implementation we used a combination
of different features. We used the 20 most frequent character
2-grams, the 20 most frequent 3-grams, the 10 most frequent
word 3-grams and the 10 most frequent \gls{POS} tag 3-grams.
We used the random forest implementation from \texttt{sklearn}
\footnote{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html}.

We tested our solution by splitting the training data into two sets a set used
for training and a set used for testing. For this test we split with 80\% being
the training set, and the rest being the test set. We computed the average of
100 tests to account for randomness. The accuracy on the 2015 dataset resulted
only in 0.48 accuracy.
