\section{Method}

\subsection{Text Features}
Feature extraction from a text is the process of finding a vector that
represents the text. There are a lot of different features that can be extracted
and we use several of them. Specifically we use some character level features,
some word level features and some \gls{POS} tagging features. In this section
we will define the different features we use. The specific features used in
different experiments will be described under those experiments.

N-grams are subsequences extracted from a sequence of tokens. For example
3-grams is all subsequences of length three of a given sequence. Using
individual characters as tokens all \textit{character 3-grams} of the string
"hello" is "hel", "ell" and "llo". We use several different types of n-grams
in different experiments including character n-grams, word n-grams, special
character n-grams and \gls{POS} tagging n-grams. Word n-grams are subsequences
of words, special character n-grams are subsequences of characters with
alphanumeric characters removed, \gls{POS} tagging n-grams are subsequences of
\gls{POS} tags. A special case of n-grams is 1-grams which is just a count of
the different tokens in a sequence. We will refer to 1-grams as frequencies.

\subsection{Delta Method} \label{subsec: Delta Method}
We have chosen to use the Delta method as a baseline method for our other
implementations. The Delta method is described by \cite{evert2015towards} and
consist of extracting word frequencies, applying a linear transformation to
those frequencies and using \gls{KNN} with different distance metrics. There are
a number of parameters to choose. First of all the number of different words to
find frequencies for has to be chosen (it was originally chosen at 150 words
\cite{evert2015towards}). Then the linear transformation can be chosen, the
usual transformation is a normalization to zero mean unit variance. And finally
the distance metric can be chosen.

\subsection{Generalising Random Forest} \label{subsec:Generalising Random Forest}
In addition to the delta method we chose to use the Random Forest approach
suggested by \cite{pacheco2015}. In our implementation we use different features
than in the their proposal. The idea of the method is to learn a general
difference between feature vectors of texts written by the same author and
feature vectors written by the different authors. The generalization is obtained
by combining the vectors of known texts with vectors of unknown text and
training on that combination instead of on the raw vectors. Rewriting the
encoding function used in \cite{pacheco2015}, we get the following combining
function,

\begin{equation}
        R_{i_k} = \frac{(A_{i_k}-U_{i_k})^2+1}{(B_k-U_{i_k})^2+1}
\end{equation}

Where $A$ is an author-specific text, $U$ is an author-unknown text, $B$ is a
\gls{UBM} describing a general author independent text. $i$ denotes from which
datapoint we get our author-specific and non-specific texts, and $k$ denotes a
specific feature of a given text. As such $R_{i_k}$ describes the encoding of a
specific feature for a specific data point.

The \gls{UBM} is meant to represent the features of an author independent text.
It is computed by concatenating all texts in the training dataset and computing
features from that resulting text. Since multiple authors is then part of the
text we are computing features from, the assumption is that the author specific
features will be averaged out and the \gls{UBM} will represent the features of
an author independent text. The addition of 1 in the model prevent division
by zero. Since 1 is added both in the numerator and the denominator it does
not otherwise change the result. The squaring of $(A_i - U_i)$ and $(B - U_i)$
both prevent negative values and punishes values that are far away. Therefore
each value in the resulting encoded feature vector is in the range $[0; \inf[$.

Let's examine what the above equation describes. Fix any specific $i$ and let
$A$ be $A_i$ and $U$ be $U_i$ then for each feature $k$ we compute,

\begin{equation}
\label{eq:rf-encode}
    R_k = \frac{(A_k-U_k)^2+1}{(B_k-U_k)^2+1},
\end{equation}

The $k$'th feature in each of the vectors is the same feature just extracted
from different texts. When the feature of the unknown text $U_k$ is closer to
$A_k$ than $B_k$ the numerator in the fraction will be greater than the
denominator giving us something in the range $[0; 1]$. When the feature of the
unknown text $U_k$ is closer to $B_k$ than $A_k$ the numerator will be lesser
than the denominator and we will therefore get a value in the interval
$[1; \infty[$. That results in $R$ being a vector containing values from 0 to
$\infty$ where it is between 0 and 1 whenever a feature is closer to the author
specific text than the universal text and greater than 1 otherwise.

The random forest algorithm is then trained on these encoded feature vectors
where it is supposed to learn a general difference between feature vectors of
the same authors and feature vectors of different authors.

\subsection{Extended Delta}
As described earlier there are many ways to extend and change the delta method.
We tried both using different features and different distance measures to obtain
better results than the original delta method. The different feature
combinations were tried experimentally one after the other. If a feature
combination did well on a dataset we tried adding or removing features from that
feature set to maybe obtain something better. The features we tried were
different combinations of character n-grams, word n-grams and \gls{POS}-tag
n-grams.

The different distance measures we tried were the Manhattan distance and the
Euclidean distance.

% TODO: Justify choices of distances.

\subsection{Author Specific SVM} \label{subsec:author_specific_svm}
We implemented an approach using \gls{SVM}'s inspired by \cite{hansen2014}. The
approach is only applicable to problems with more than a single text per author.
The classification in this approach is done by training an \gls{SVM} classifier
on all known texts of an author and an equal number of texts from other authors.
Then the unknown text is given to the \gls{SVM} and is classified either as
belonging to the same author or as belonging to a different author.

If there is only a single known text available for an author it does not make
sense to train an \gls{SVM} since there is simply to little data.

\subsection{Experiments}
In this section we describe the different experiments we have performed.
We have tested the different methods we have implemented with different
features and on different datasets. In our experiments we use data from PAN
2013 \footnote{http://pan.webis.de/clef13/pan13-web/index.html} and PAN 2015
\footnote{http://pan.webis.de/clef15/pan15-web/index.html}.

The PAN 2013 data consist of texts from English, Greek and Spanish authors. We
work only on the English texts which consists of texts from 10 authors. For
each author there is (between 1 and 10) known texts and a single unknown text.
The task is given the known texts of an author to determine whether or not the
unknown text is written by the same author or not.

The PAN 2015 data consist of texts by authors in English, Dutch, Greek and
Spanish. Again we only work with English texts. Unlike the 2013 dataset there is
only a single known text for each author and a single unknown text. There is
therefore much less known data available for each author which makes the
verification harder. In the 2015 dataset there is 100 \textit{problems} in which
some of the known texts are the same (i.e. there is not 100 authors).

To generate metadata about English texts we use the Brown dataset
\footnote{http://clu.uni.no/icame/brown/bcm-los.html}. The brown dataset
contains more than 1,000,000 words across different genres and is therefore
perfect to use for our datasets. We specifically use the dataset to identify
the most frequently used n-grams (of all kinds). If we were to use our training
data to generate that we would risk having a bias towards our specific training
dataset.

We will evaluate the performance of the different algorithms on the training
data with the accuracy of the algorithms. The accuracy is computed as the number
of correct answers divided by the total number of problems. For some methods we
will also report the \gls{TPR} and \gls{TNR}. The \gls{TPR} and \gls{TNR} are
defined as,

\begin{align}
    TPR &= \frac{\text{true positives}}{\text{true positives} +
        \text{false negatives}} \\
    TNR &= \frac{\text{true negatives}}{\text{true negatives} +
        \text{false positives}}
\end{align}

\subsubsection{Delta Method}
In the delta method we work only with word frequencies as originally proposed.
As the linear function we normalize to 0 mean and unit variance and as features
we use the $n$ most frequent words. We get the most frequent words by using the
brown dataset. In the \gls{KNN} part we use the Manhatten distance and only a
single nearest neighbour since in one of the datasets we have only 1 text for
each author. To classify the unknown texts as either written by or not written
by the author we train a \gls{KNN} for each unknown text. Each classifier is
trained with a known text from the author in question and $m$ other random
texts. If the unknown text is classified as belonging to one of the $m$ random
authors instead of the author in question we report that the unknown text is not
written by the author. If the text is classified as belonging to the author in
question we classify it as being written by the author.

We chose the number of most frequent words $n$ and number of opposing
authors $m$ by trying different configurations in a grid and choosing the
best values. Each configuration is tried 100 times since random authors are
chosen in each run. On the training dataset we obtained the results shown in
Table \ref{fig:delta_pan_2013_res} for the PAN 2013 data and in Table
\ref{fig:delta_pan_2015_res} for the PAN 2015 data. For PAN 2013 the results are
generally better since there is more text available and the best accuracy were
obtained when using the 300 most frequent words and 4 opposing authors. For PAN
2015 the best result is obtained when using the 200 most frequent words and 1
opposing authors.

\begin{table}
    \centering
    \begin{tabular}{c|lccccc}
               &                   & $n=100$ & $n=200$ & $n=300$ & $n=400$ & $n=500$ \\
        \hline
        $m=1$  & \textbf{Accuracy} & 0.62093 & 0.64437 & 0.65062 & 0.66718 & 0.66281 \\
               & \textbf{TPR}      & 0.75812 & 0.79562 & \textbf{0.80812} & 0.79750 & 0.77500 \\
               & \textbf{TNR}      & 0.48375 & 0.49312 & 0.49312 & 0.53687 & 0.55062 \\
        \hline
        $m=2$  & \textbf{Accuracy} & 0.65375 & 0.68562 & 0.69593 & 0.68968 & 0.68250 \\
               & \textbf{TPR}      & 0.63250 & 0.71875 & 0.74000 & 0.70062 & 0.67125 \\
               & \textbf{TNR}      & 0.67500 & 0.65250 & 0.65187 & 0.67875 & 0.69375 \\
        \hline
        $m=3$  & \textbf{Accuracy} & 0.66250 & 0.68187 & 0.69687 & 0.69250 & 0.69843 \\
               & \textbf{TPR}      & 0.55937 & 0.64125 & 0.68375 & 0.65437 & 0.63375 \\
               & \textbf{TNR}      & 0.76562 & 0.72250 & 0.71000 & 0.73062 & 0.76312 \\
        \hline
        $m=4$  & \textbf{Accuracy} & 0.66312 & 0.68656 & \textbf{0.70062} & 0.68062 & 0.68281 \\
               & \textbf{TPR}      & 0.49875 & 0.61125 & 0.65125 & 0.61875 & 0.57562 \\
               & \textbf{TNR}      & 0.82750 & 0.76187 & 0.75000 & 0.74250 & 0.79000 \\
        \hline
        $m=5$  & \textbf{Accuracy} & 0.66343 & 0.69468 & 0.69250 & 0.66937 & 0.68437 \\
               & \textbf{TPR}      & 0.46687 & 0.59375 & 0.61812 & 0.57500 & 0.54937 \\
               & \textbf{TNR}      & 0.86000 & 0.79562 & 0.76687 & 0.76375 & 0.81937 \\
        \hline
        $m=6$  & \textbf{Accuracy} & 0.63531 & 0.67437 & 0.69406 & 0.67312 & 0.66718 \\
               & \textbf{TPR}      & 0.39937 & 0.55125 & 0.59750 & 0.56750 & 0.50312 \\
               & \textbf{TNR}      & 0.87125 & 0.79750 & 0.79062 & 0.77875 & 0.83125 \\
        \hline
        $m=7$  & \textbf{Accuracy} & 0.63843 & 0.67906 & 0.68437 & 0.67000 & 0.66562 \\
               & \textbf{TPR}      & 0.37875 & 0.53812 & 0.58500 & 0.54687 & 0.49625 \\
               & \textbf{TNR}      & 0.89812 & 0.82000 & 0.78375 & 0.79312 & 0.83500 \\
        \hline
        $m=8$  & \textbf{Accuracy} & 0.63406 & 0.69281 & 0.68156 & 0.65218 & 0.67156 \\
               & \textbf{TPR}      & 0.36937 & 0.55062 & 0.56250 & 0.52250 & 0.50000 \\
               & \textbf{TNR}      & 0.89875 & 0.83500 & 0.80062 & 0.78187 & 0.84312 \\
        \hline
        $m=9$  & \textbf{Accuracy} & 0.63781 & 0.68031 & 0.66437 & 0.66656 & 0.67031 \\
               & \textbf{TPR}      & 0.36625 & 0.51937 & 0.53000 & 0.52562 & 0.47375 \\
               & \textbf{TNR}      & \textbf{0.90937} & 0.84125 & 0.79875 & 0.80750 & 0.86687 \\
        \hline
        $m=10$ & \textbf{Accuracy} & 0.62562 & 0.68781 & 0.68343 & 0.67687 & 0.66000 \\
               & \textbf{TPR}      & 0.34625 & 0.51375 & 0.54937 & 0.53125 & 0.46312 \\
               & \textbf{TNR}      & 0.90500 & 0.86187 & 0.81750 & 0.82250 & 0.85687
    \end{tabular}
    \caption{Accuracy, \gls{TPR} and \gls{TNR} on different number of most
        frequent words $n$ and different number of opposing authors $m$ for the
        Delta Method. Each number is an average of 100 runs since there is
        randomness involved when picking the opposing authors. The test is run
        on the PAN 2013 dataset. Maximum values for both Accuracy, \gls{TPR} and
        \gls{TNR} is shown in bold.}
    \label{fig:delta_pan_2013_res}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{c|lccccc}
               &                   & $n=100$ & $n=200$ & $n=300$ & $n=400$ & $n=500$ \\
        \hline
        $m=1$  & \textbf{Accuracy} & 0.56950 & \textbf{0.60239} & 0.56210 & 0.57269 & 0.56170 \\
               & \textbf{TPR}      & 0.58740 & \textbf{0.64660} & 0.61759 & 0.62200 & 0.61020 \\
               & \textbf{TNR}      & 0.55160 & 0.55820 & 0.50659 & 0.52340 & 0.51320 \\
        \hline
        $m=2$  & \textbf{Accuracy} & 0.56810 & 0.58970 & 0.56470 & 0.57290 & 0.56030 \\
               & \textbf{TPR}      & 0.42219 & 0.46740 & 0.45720 & 0.45340 & 0.44640 \\
               & \textbf{TNR}      & 0.71400 & 0.71200 & 0.67220 & 0.69239 & 0.67420 \\
        \hline
        $m=3$  & \textbf{Accuracy} & 0.55399 & 0.57510 & 0.56220 & 0.56850 & 0.55380 \\
               & \textbf{TPR}      & 0.32480 & 0.36880 & 0.36060 & 0.36660 & 0.33820 \\
               & \textbf{TNR}      & 0.78319 & 0.78140 & 0.76379 & 0.77040 & 0.76940 \\
        \hline
        $m=4$  & \textbf{Accuracy} & 0.54589 & 0.56390 & 0.54560 & 0.55580 & 0.54710 \\
               & \textbf{TPR}      & 0.25920 & 0.29940 & 0.29460 & 0.29000 & 0.27800 \\
               & \textbf{TNR}      & 0.83260 & 0.82840 & 0.79660 & 0.82160 & 0.81620 \\
        \hline
        $m=5$  & \textbf{Accuracy} & 0.53520 & 0.55430 & 0.54150 & 0.56290 & 0.54960 \\
               & \textbf{TPR}      & 0.21000 & 0.24920 & 0.24420 & 0.25860 & 0.24220 \\
               & \textbf{TNR}      & 0.86040 & 0.85939 & 0.83880 & 0.86720 & 0.85700 \\
        \hline
        $m=6$  & \textbf{Accuracy} & 0.52749 & 0.54820 & 0.53709 & 0.55530 & 0.54260 \\
               & \textbf{TPR}      & 0.17980 & 0.21840 & 0.21800 & 0.23100 & 0.20739 \\
               & \textbf{TNR}      & 0.87520 & 0.87800 & 0.85620 & 0.87959 & 0.87780 \\
        \hline
        $m=7$  & \textbf{Accuracy} & 0.52790 & 0.54189 & 0.53240 & 0.54550 & 0.54530 \\
               & \textbf{TPR}      & 0.16240 & 0.19160 & 0.19440 & 0.18660 & 0.19020 \\
               & \textbf{TNR}      & 0.89340 & 0.89220 & 0.87040 & 0.90440 & 0.90040 \\
        \hline
        $m=8$  & \textbf{Accuracy} & 0.52340 & 0.53570 & 0.52459 & 0.54899 & 0.53419 \\
               & \textbf{TPR}      & 0.14820 & 0.17440 & 0.16180 & 0.17760 & 0.15560 \\
               & \textbf{TNR}      & 0.89860 & 0.89700 & 0.88740 & 0.92040 & 0.91280 \\
        \hline
        $m=9$  & \textbf{Accuracy} & 0.51930 & 0.53340 & 0.52060 & 0.54229 & 0.53460 \\
               & \textbf{TPR}      & 0.13040 & 0.15220 & 0.15200 & 0.15700 & 0.15060 \\
               & \textbf{TNR}      & 0.90819 & 0.91460 & 0.88920 & 0.92760 & 0.91860 \\
        \hline
        $m=10$ & \textbf{Accuracy} & 0.51780 & 0.52550 & 0.52260 & 0.54080 & 0.53150 \\
               & \textbf{TPR}      & 0.11860 & 0.13760 & 0.13960 & 0.13660 & 0.12940 \\
               & \textbf{TNR}      & 0.91700 & 0.91340 & 0.90560 & \textbf{0.94500} & 0.93360
    \end{tabular}
    \caption{Accuracy, \gls{TPR} and \gls{TNR} on different number of most
        frequent words $n$ and different number of opposing authors $m$ for the
        Delta Method. Each number is an average of 100 runs since there is
        randomness involved when picking the opposing authors. The test is run
        on the PAN 2015 dataset. Maximum values for both Accuracy, \gls{TPR} and
        \gls{TNR} is shown in bold.}
    \label{fig:delta_pan_2015_res}
\end{table}

\subsubsection{Generalising Random Forest}
In the \nameref{subsec:Generalising Random Forest}, we have the possibility
to use of a wide variety of features. It's a random subset of these features
which are then used to train each of trees trees in our random forest. There are
no limit to how many and what features can be used, thus the task of finding
a good/perfect input to train out forest on is quite comprehensive. As such
only some preliminary tests have been perform, to get a general idea of the
efficiency of the method. There will however be expanded on this in the future.

The \texttt{sklearn}
\footnote{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.Rand
omForestClassifier.html} offers a wide variety of configurations with regards
to how our forest is built. Most configuration was set to its' default value,
with the exception of \texttt{n\_estimators}.
\texttt{n\_estimators} denotes how many decision trees are in our forest, and
due to ensemble nature of the algorithm, we can increase this parameter to
create an average classification prediction based on a larger set of individual
predictions, thus decreasing the overall variance our model. In the following
tests, \texttt{n\_estimators} is set to 100. The payoff when increasing
the number of trees used, is the runtime. Having 1000 trees in your forest
results in a slower runtime, compared to 100. The increase in trees also have
diminishing returns on the decrease of variance, so at some point increasing
the trees doesn't make a lot of sense, as the variance is only altered very
little with each tree. 1000 Trees was the amount we deemed to adequately decrease
the variance of the forest, while still running at an acceptable speed. 

The dataset in these experiments were split into a test and a validation set.
The selection was done by randomly shuffling the entire data set and taking
the first 80\% of the dataset as the training set, and the last 20\% as the
validation set. 
As such we need a another layer of averaging, 
to combat the variance introduced by the random shuffling. 
In the following cases, this is done over 100 iterations 
of random forest predictions.\\\\

In the following, we created our \gls{UBM} using the concatenation of all
author-specific texts, from our training data set.

In terms of features, we chose to feed our random forest algorithm a large set of 
features with different focuses. The algorithm is going to pick and chose features, 
based on their impact on the actual classification. The low-impact features aren't 
adding any noise, so we might as well feed the algorithm as many features as possible, 
and then let it make the decision based on their individual impact.
We chose to use the following features:\\\\
50 Most Frequent $[2,...,5]$ Word-N-Grams\\
50 Most Frequent $[2,...,5]$ Character-N-Grams\\ 
50 Most Frequent $[2,...,5]$ PosTag-N-Grams\\
5 Most Frequent 2 and 3 Special-Character-N-Grams\\
50 Most Frequent Words\\\\

In order to achieve better generalization, the brown corpus was used as the bases for the generation of the features.\\\\

When applying the algorithm to the 2015, and using the \gls{UBM} style encoding, we got an 
accuracy of 0.6015, or 60.15\%.
Looking the feature influences averaged over all 100 model iterations, we can see that the top 
features are mostly dominated by the ones who are character based. While the word-n-grams have 
little to no influence on the outcome of the predictions. The suspected reason for this is the 
small amount of words in each text of the 2015 data set. This means that the sequences of words 
that are present, are very rare, and thus has little influence. This point is backed up by that 
fact that word-1-grams does actually have an influence, as the capture the use of stop words, 
such as "for" and "has in this case, regardless of the subject of the text.\\\\
In addition to using the \gls{UBM} based encoding, another one was used used as well, simply 
replacing equation \eqref{eq:rf-encode}.
$$
R_k = A_k - U_k
$$
This yielded an accuracy of 0.5675. This was again dominated by character features, when looking 
at the final feature importance. However pos-tags had slightly more impact.

\subsubsection{Extended Delta}
We tried different feature combinations and distances. The results of running on
the training data is shown in Table
\ref{fig:extended_delta_method_manhattan_result} for the Manhattan distance and
in Table \ref{fig:extended_delta_method_euclidean_result} for the Euclidean
distance.

\begin{landscape}
\begin{table}
    \small
    \centering
    \begin{tabular}{llll|ll}
        % Header.
        \textbf{Character n-grams} & \textbf{Word n-grams} &
        \textbf{Pos-tag n-grams} & \textbf{Special character n-grams} &
        \textbf{PAN 2015 result} & \textbf{PAN 2013 result} \\
        \hline
        % Test 1.
        100 2-, 3-, 4-, 5-grams & 5 1-, 2-, 3-, 4-grams &
        10 1-, 2-, 3-, 4-grams & 5 1-, 2-, 3-grams & 0.54680, 3 & 0.68031, 3 \\
        % Test 2.
        100 2-, 3-, 4-grams & 300 1-grams & NONE & NONE & 0.56340, 2 &
        \textbf{0.73718, 5} \\
        % Test 3.
        100 2-, 3-, 4-grams & NONE & NONE & 20 2-, 3-, 4-grams &
        0.56980, 2 & 0.68500, 3 \\
        % Test 4.
        100 2-, 3-, 4-grams & NONE & 15 1-, 2-, 3-grams & 20 2-, 3-, 4-grams &
        0.58029, 3 & 0.67718, 8 \\
        % Test 5.
        NONE & NONE & 15 1-, 2-, 3-grams & NONE & 0.51470, 10 & 0.62062, 8 \\
        % Test 6.
        20 2-, 3-, 4-, 5-, 6-grams & NONE & NONE & NONE & 0.54640, 4 &
        0.69000, 8 \\
        % Test 7.
        150 2-, 3-grams & NONE & NONE & NONE & 0.56370, 4 &
        \textbf{0.70531, 4} \\
        % Test 8.
        NONE & 50 2-, 3-grams & NONE & NONE & 0.52120, 9 & 0.60812, 3 \\
        % Test 9.
        NONE & 20 2-, 3-grams & NONE & NONE & 0.48409, 10 & 0.58187, 4 \\
        % Test 10.
        500 4-grams & NONE & NONE & NONE & 0.57400, 3 & \textbf{0.72125, 3} \\
        % Test 11.
        300 4-grams & NONE & NONE & NONE & 0.55450, 4 & \textbf{0.72937, 3} \\
        % Test 12.
        150 4-grams & NONE & NONE & NONE & 0.52630, 9 & 0.65093, 4 \\
        % Test 13.
        500 3-grams & 300 1-grams & NONE & NONE & 0.59750, 2 & 0.70031, 4 \\
        % Test 14.
        300 3-grams & 300 1-grams & NONE & NONE & 0.57139, 1 & 0.69218, 2 \\
        % Test 15.
        1000 3-grams & NONE & NONE & NONE & 0.59390, 2 & \textbf{0.70812, 10} \\
        % Test 16.
        1000 4-grams & NONE & NONE & NONE & 0.57480, 3 & \textbf{0.72125, 5} \\
        % Test 17.
        1000 3-, 4-, 5-grams & NONE & NONE & NONE & 0.56220, 2 & 0.69625, 3 \\
        % Test 18.
        NONE & NONE & NONE & 20 1-, 2-, 3-grams & \textbf{0.62780, 2} &
        0.64312, 10 \\
        % Test 19.
        NONE & NONE & NONE & 10 1-, 2-, 3-grams & \textbf{0.61879, 3} &
        0.549375, 8 \\
        % Test 20.
        10 2-, 3-, 4-grams & NONE & NONE & 10 1-, 2-, 3-grams & 0.58980, 3 &
        0.69406, 8
    \end{tabular}
    \caption{Results of different feature combinations with the Delta method
    using the Manhattan distance. The result consist of 2 numbers in the format
    $a, b$. $a$ corresponds to the accuracy obtained with the configuration and
    $b$ is the number of opponents that obtained that accuracy. Results that
    beat our baseline results are shown in bold.}
    \label{fig:extended_delta_method_manhattan_result}
\end{table}
\end{landscape}

\begin{landscape}
\begin{table}
    \small
    \centering
    \begin{tabular}{llll|ll}
        % Header.
        \textbf{Character n-grams} & \textbf{Word n-grams} &
        \textbf{Pos-tag n-grams} & \textbf{Special character n-grams} &
        \textbf{PAN 2015 result} & \textbf{PAN 2013 result} \\
        \hline
        % Test 1.
        100 2-, 3-, 4-, 5-grams & 5 1-, 2-, 3-, 4-grams &
        10 1-, 2-, 3-, 4-grams & 5 1-, 2-, 3-grams & 0.54570, 7 & 0.67750, 5 \\
        % Test 2.
        100 2-, 3-, 4-grams & 300 1-grams & NONE & NONE & 0.54080, 2 &
        \textbf{0.72125, 4} \\
        % Test 3.
        100 2-, 3-, 4-grams & NONE & NONE & 20 2-, 3-, 4-grams &
        0.5618, 3 & 0.65812, 5 \\
        % Test 4.
        100 2-, 3-, 4-grams & NONE & 15 1-, 2-, 3-grams & 20 2-, 3-, 4-grams &
        0.56500, 6 & 0.68031, 4 \\
        % Test 5.
        NONE & NONE & 15 1-, 2-, 3-grams & NONE & 0.52110, 10 & 0.61156, 3 \\
        % Test 6.
        20 2-, 3-, 4-, 5-, 6-grams & NONE & NONE & NONE & 0.555, 3 &
        0.66656, 2 \\
        % Test 7.
        150 2-, 3-grams & NONE & NONE & NONE & 0.55460, 3 &
        \textbf{0.70343, 9} \\
        % Test 8.
        NONE & 50 2-, 3-grams & NONE & NONE & 0.52200, 9 & 0.56687, 3 \\
        % Test 9.
        NONE & 20 2-, 3-grams & NONE & NONE & 0.48530, 9 & 0.58718, 3 \\
        % Test 10.
        500 4-grams & NONE & NONE & NONE & 0.55830, 3 & \textbf{0.70468, 8} \\
        % Test 11.
        300 4-grams & NONE & NONE & NONE & 0.52680, 3 & \textbf{0.71875, 6} \\
        % Test 12.
        150 4-grams & NONE & NONE & NONE & 0.53669, 6 & 0.64281, 7 \\
        % Test 13.
        500 3-grams & 300 1-grams & NONE & NONE & 0.5538, 2 & 0.69343, 6 \\
        % Test 14.
        300 3-grams & 300 1-grams & NONE & NONE & 0.53400, 2 &
        \textbf{0.71437, 6} \\
        % Test 15.
        1000 3-grams & NONE & NONE & NONE & 0.54910, 2 & \textbf{0.72156, 4} \\
        % Test 16.
        1000 4-grams & NONE & NONE & NONE & 0.57100, 2 & \textbf{0.72125, 3} \\
        % Test 17.
        1000 3-, 4-, 5-grams & NONE & NONE & NONE & 0.54579, 4 &
        \textbf{0.71968, 4} \\
        % Test 18.
        NONE & NONE & NONE & 20 1-, 2-, 3-grams & \textbf{0.61200, 2} &
        0.59375, 8 \\
        % Test 19.
        NONE & NONE & NONE & 10 1-, 2-, 3-grams & \textbf{0.61370, 3} &
        0.550625, 9 \\
        % Test 20.
        10 2-, 3-, 4-grams & NONE & NONE & 10 1-, 2-, 3-grams & 0.57980, 3 &
        0.68125, 10
    \end{tabular}
    \caption{Results of different feature combinations with the Delta method
    using the Euclidean distance.  The result consist of 2 numbers in the format
    $a, b$. $a$ corresponds to the accuracy obtained with the configuration and
    $b$ is the number of opponents that obtained that accuracy. Results that
    beat our baseline results are shown in bold.}
    \label{fig:extended_delta_method_euclidean_result}
\end{table}
\end{landscape}

\subsubsection{Author Specific SVM} \label{subsec:author_specific_svm_impl}
In this approach we have experimented with a couple of different feature
configurations. Configuration (A) consists of the 500 most frequent 3-, 4-
and 5-character-grams, the 100 most frequent 3- and 4-word-grams, the 20
most frequent 2-, 3- and 4-postag-grams. Configuration (B) consists of the
frequencies of the 300 most frequent words. The most frequent n-grams is found
in the brown text corpus. We test only on the PAN 2013 dataset since the PAN
2015 dataset contains only a single known text per author and an SVM cannot
train with a single datapoint in each class. We use the \textit{sklearn}
implementation of \gls{SVM}'s which internally use \textit{libsvm}.

We use the RBF kernel and we choose hyperparameters via cross validation.
Each configuration of features will use different hyperparameters. The cross
validation is performed by looping through the list of authors. For each
author we perform a grid search for value of $C \in \{10^{-2}, 10^{0}, \dots,
10^{10}\}$ and $\gamma \in \{10^{-9}, 10^{-7}, \dots, 10^{3}\}$. The best values
for each author are found via leave one out cross validation. The final $C$ and
$\gamma$ values are chosen as the configurations used most often by the authors.
After we have found the best hyperparameters we run the classifier over all
authors 100 times with those hyperparameters. The mean accuracy over the 100
runs is then computed.

Configuration (A) use the hyperparameters $C = 100$ and $\gamma = 0.00001$
and obtained an average accuracy of 0.84599 with a \gls{TPR} of 0.95 and a
\gls{TNR} of 0.634. Configuration (B) use the hyperparameters $C = 100$ and
$\gamma = 0.001$ and obtained an average accuracy of 0.84799 with a \gls{TPR}
of 0.99899 and a \gls{TNR} of 0.632.
