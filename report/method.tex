\section{Method}

\subsection{Text Features}
Feature extraction from a text is the process of finding a vector that
represents the text. There are a lot of different features that can be extracted
and we use several of them. Specifically we use some character level features,
some word level features and som \gls{POS} tagging features. In this section
we will define the different features we use. The specific features used in
different experiments will be described under those experiments.

N-grams are subsequences extracted from a sequence of tokens. For example
3-grams is all subsequences of length three of a given sequence. For example
using individual characters as tokens all \textit{character 3-grams} of the
string "hello" is "hel", "ell" and "llo". We use several different types of
n-grams in different experiments including character n-grams, word n-grams,
special character n-grams and \gls{POS} tagging n-grams. Word n-grams are
subsequences of words, special character n-grams are subsequences of characters
with alphanumeric characters removed, \gls{POS} tagging n-grams are subsequences
of \gls{POS} tags. A special case of n-grams is 1-grams which is just a count of
the different tokens in a sequence. We will refer to 1-grams as frequencies.

\subsection{Delta Method}
\label{subsec: Delta Method}
We have chosen to use the Delta method as a baseline method for our other
implementations. The Delta method is described by \cite{evert2015towards} and
consist of extracting word frequencies, applying a linear transformation to
those frequencies and using \gls{KNN} with different distance metrics. There are
a number of parameters to choose. First of all the number of different words to
find frequencies for has to be chosen (it was originally chosen at 150 words
\cite{evert2015towards}). Then the linear transformation can be chosen, the
usual transformation is a normalization to zero mean unit variance. And finally
the distance metric can be chosen.

\subsection{Generalising Random Forest}
\label{subsec:Generalising Random Forest}
In addition to the delta method we chose to use the Random Forest approach
suggested by \cite{pacheco2015}. In our implementation we use different
features than in the their proposal. The idea of the method is to learn a
general difference between feature vectors of texts written by the same author
and feature vectors written by the different authors. The generalization is
obtained by combining the vectors of known texts with vectors of unknown
text and training on that combination instead of on the raw vectors. Rewriting the encoding function used in \cite{pacheco2015}, we get the following combining function,

\begin{equation}
        R_{i_k} = \frac{(A_{i_k}-U_{i_k})^2+1}{(B_k-U_{i_k})^2+1}
\end{equation}

Where $A$ is an author-specific text, $U$ is an author-unknown text, $B$ is a \gls{UBM}
describing a general author independent text. $i$ denotes from which datapoint we get our author-specific and non-specific texts, and $k$ denotes a specific feature of a given text. As such $R_{i_k}$ describes the encoding of a specific feature for a specific data point.

The \gls{UBM} is meant to represent the features of an author independent text.
It is computed by concatenating all texts in the training dataset and computing
features from that resulting text. Since multiple authors is then part of the
text we are computing features from, the assumption is that the author specific
features will be averaged out and the \gls{UBM} will represent the features of
an author independent text. The addition of 1 in the model prevent division
by zero. Since 1 is added both in the numerator and the denominator it does
not otherwise change the result. The squaring of $(A_i - U_i)$ and $(B - U_i)$
prevents negative values. Therefore each value in the resulting encoded feature
vector is in the range $[0; \inf[$.

Let's examine what the above equation describes. Fix any specific $i$ and let
$A$ be $A_i$ and $U$ be $U_i$ then for each feature $k$ we compute,

\begin{equation}
\label{eq:rf-encode}
    R_k = \frac{(A_k-U_k)^2+1}{(B_k-U_k)^2+1},
\end{equation}

The $k$'th feature in each of the vectors is the same feature just extracted
from different texts. When the feature of the unknown text $U_k$ is closer to
$A_k$ than $B_k$ the numerator in the fraction will be greater than the
denominator giving us something in the range $[0; 1]$. When the feature of the
unknown text $U_k$ is closer to $B_k$ than $A_k$ the numerator will be lesser
than the denominator and we will therefore get a value in the interval
$[1; \infty[$. That results in $R$ being a vector containing values from 0 to
$\infty$ where it is between 0 and 1 whenever a feature is closer to the author
specific text than the universal text and greater than 1 otherwise.

The random forest algorithm is then trained on these encoded feature vectors
where it is supposed to learn a general difference between feature vectors of
the same authors and feature vectors of different authors.

\subsection{Extended Delta}
% TODO write.

\subsection{Author Specific SVM}
We implemented an approach using \gls{SVM}'s inspired by \cite{hansen2014}. The
approach is only applicable to problems with more than a single text per
unknown author. The classification in this approach is done by training an \gls{SVM}
classifier on all known texts of an author and an equal number of texts from
other authors. Then the unknown text is given to the \gls{SVM} and is classified
either as belonging to the same author or as belonging to a different author.

If there is only a single known text available for an author it does not make
sense to train an \gls{SVM} since there is simply to little data.

\subsection{Author Specific Random Forest}
% TODO: write.

\subsection{Experiments}
In this section we describe the different experiments we have performed.
We have tested the different methods we have implemented with different
features and on different datasets. In our experiments we use data from PAN
2013 \footnote{http://pan.webis.de/clef13/pan13-web/index.html} and PAN 2015
\footnote{http://pan.webis.de/clef15/pan15-web/index.html}. The PAN 2013 data
consist of texts from English, Greek and Spanish authors. We work only on the
English texts which consist of texts from 10 authors. There is a number (between
1 and 10) known texts from each author and a single unknown text. The task is
given the known texts of an author to determine whether or not the unknown text
is written by the same author or not.

The PAN 2015 data consist of texts by authors in English, Dutch, Greek and
Spanish. Again we only work with English texts. Unlike the 2013 dataset there is
only a single known text for each author and a single unknown text. There is
therefore much less known data available for each author which makes the
verification harder. In the 2015 dataset there is 100 \textit{problems} in which
some of the known texts are the same (i.e. there is not 100 authors).

% TODO: Describe brown.

% TODO Change delta to brown.
\subsubsection{Delta Method}
In the delta method we work only with word frequencies as originally proposed.
As the linear function we normalize to 0 mean and unit variance and as features
we use the $n$ most frequent words. We get the most frequent words by using the
brown dataset. In the \gls{KNN} part we use the Manhatten distance and only a
single nearest neighbour since in one of the datasets we have only 1 text for
each author. To classify the unknown texts as either written by or not written
by the author we train a \gls{KNN} for each unknown text. Each classifier is
trained with a known text from the author in question and $m$ other random
texts. If the unknown text is classified as belonging to one of the $m$ random
authors instead of the author in question we report that the unknown text is not
written by the author. If the text is classified as belonging to the author in
question we classify it as being written by the author.

We chose the number of most frequent words $n$ and number of opposing
authors $m$ by trying different configurations in a grid and choosing the
best values. On the training dataset we obtained the results shown in
Figure \ref{fig:delta_pan_2013_res} for the PAN 2013 data and in Figure
\ref{fig:delta_pan_2015_res} for the PAN 2015 data. For PAN 2013 the results are
generally better since there is more text available and the best accuracy were
obtained when using the 300 most frequent words and 3 opposing authors. For PAN
2015 the best result is obtained when using the 200 most frequent words and 1
opposing authors.

\begin{figure}
    \centering
    \begin{tabular}{c|ccccc}
               & $n=100$ & $n=200$ & $n=300$ & $n=400$ & $n=500$ \\
        \hline
        $m=1$  & 0.61062 & 0.64718 & 0.65875 & 0.65906 & 0.65718 \\
        $m=2$  & 0.64593 & 0.68781 & 0.70281 & 0.68906 & 0.69593 \\
        $m=3$  & 0.65625 & 0.68187 & \textbf{0.71156} & 0.70125 & 0.68718 \\
        $m=4$  & 0.66156 & 0.69875 & 0.70281 & 0.67781 & 0.68187 \\
        $m=5$  & 0.64843 & 0.68406 & 0.70625 & 0.68218 & 0.67750 \\
        $m=6$  & 0.65125 & 0.68906 & 0.70156 & 0.67500 & 0.67000 \\
        $m=7$  & 0.65093 & 0.68375 & 0.70125 & 0.67062 & 0.66812 \\
        $m=8$  & 0.62593 & 0.67125 & 0.68812 & 0.66468 & 0.67187 \\
        $m=9$  & 0.62968 & 0.68125 & 0.69125 & 0.67218 & 0.67593 \\
        $m=10$ & 0.62593 & 0.68685 & 0.69625 & 0.66906 & 0.66156
    \end{tabular}
    \caption{Accuracy on different number of most frequent words $n$ and
        different number of opposing authors $m$ for the Delta Method. Each
        number is an average of 100 runs since there is randomness involved when
        picking the opposing authors. The test is run on the PAN 2013 dataset.}
    \label{fig:delta_pan_2013_res}
\end{figure}

\begin{figure}
    \centering
    \begin{tabular}{c|ccccc}
               & $n=100$ & $n=200$ & $n=300$ & $n=400$ & $n=500$ \\
        \hline
        $m=1$  & 0.57370 & \textbf{0.59350} & 0.56730 & 0.56470 & 0.55650 \\
        $m=2$  & 0.56800 & 0.58400 & 0.56530 & 0.56960 & 0.55190 \\
        $m=3$  & 0.55369 & 0.57580 & 0.55540 & 0.56010 & 0.54990 \\
        $m=4$  & 0.54259 & 0.56360 & 0.55140 & 0.56590 & 0.54599 \\
        $m=5$  & 0.53650 & 0.55890 & 0.54089 & 0.55950 & 0.54450 \\
        $m=6$  & 0.52900 & 0.54680 & 0.53280 & 0.55170 & 0.53770 \\
        $m=7$  & 0.52599 & 0.53750 & 0.52879 & 0.54850 & 0.53629 \\
        $m=8$  & 0.52169 & 0.53450 & 0.52950 & 0.54560 & 0.53740 \\
        $m=9$  & 0.51999 & 0.53420 & 0.52490 & 0.54829 & 0.53620 \\
        $m=10$ & 0.51960 & 0.52660 & 0.51979 & 0.54470 & 0.53510
    \end{tabular}
    \caption{Accuracy on different number of most frequent words $n$ and
        different number of opposing authors $m$ for the Delta Method. Each
        number is an average of 100 runs since there is randomness involved when
        picking the opposing authors. The test is run on the PAN 2015 dataset.}
    \label{fig:delta_pan_2015_res}
\end{figure}

% TODO: Describe number of trees and other parameters.
% TODO: Tables of tests.
\subsubsection{Generalising Random Forest}
In the \nameref{subsec:Generalising Random Forest}, we have the possibility to use of a wide variety of features. It's a random subset of these features which are then used to train each of trees trees in our random forest. There are no limit to how many and what features can be used, such the task of finding a good/perfect input to train out forest on is quite comprehensive. As such only some preliminary tests have been perform, to get a general idea of the efficiency of the method.

The \texttt{sklearn}\footnote{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html} offers a wide variety of configurations with regards to how our forest is built. Each configuration was set to its' default value, besides the \texttt{n\_estimators}. This denotes how many decision are in our forest, and due to ensemble nature of the algorithm, we can increase this parameter to create an average classification prediction based on a larger set of individual predictions, thus decreasing the overall variance our model. This also removes the need for us to compute the average ourselves, as was done in \ref{subsec: Delta Method}. In the follows tests, set \texttt{n\_estimators} to 10000.

The data set in these experiments, were split into a test and a validation set, by randomly shuffling the entire data set, and taking the first 80\% of the data set as the training set, and the last 20\% as the validation set.

In the following, we created our \gls{UBM} using the concatenation of all author-specific texts, from our training data set. Results describe number of correct predictions over total number of training points, and was run on the PAN 2015 dataset. 

$$
\left.
                \begin{array}{ll}
                  \text{60 Most Frequent Char 2 and 3-grams}\\
                  \text{60 Most Frequent Special Char 2 and 3-grams}\\
                  \text{20 Most Frequent Pos Tag 2 and 3-grams}
                \end{array}
              \right\rbrace = 0.5
$$

$$
\left.
                \begin{array}{ll}
                  \text{20 Most Frequent Word 2 and 3-grams}\\
                  \text{150 Highest word frequencies}
                \end{array}
              \right\rbrace = 0.6
$$

$$
\left.
                \begin{array}{ll}
                  \text{60 Most Frequent Char 2 and 3-grams}\\
                  \text{60 Most Frequent Special Char 2 and 3-grams}\\
                  \text{20 Most Frequent Pos Tag 2 and 3-grams}\\
                  \text{20 Most Frequent Word 2 and 3-grams}\\
                  \text{150 Highest word frequencies}
                \end{array}
              \right\rbrace = 0.55
$$

In addition to using the encoding method presented by \cite{pacheco2015}, we also attempted an alternate way of encoding the features, replacing equation \ref{eq:rf-encode}.
$$
R_k = A_k - U_k
$$
As such, the features we train our random forest on, is simply the difference in value between the features for the known and the unknown text. Using the same parameters as above but changing out encoding approach, we get the following:
$$
\left.
                \begin{array}{ll}
                  \text{60 Most Frequent Char 2 and 3-grams}\\
                  \text{60 Most Frequent Special Char 2 and 3-grams}\\
                  \text{20 Most Frequent Pos Tag 2 and 3-grams}
                \end{array}
              \right\rbrace = 0.45
$$

$$
\left.
                \begin{array}{ll}
                  \text{20 Most Frequent Word 2 and 3-grams}\\
                  \text{150 Highest word frequencies}
                \end{array}
              \right\rbrace = 0.5
$$

$$
\left.
                \begin{array}{ll}
                  \text{60 Most Frequent Char 2 and 3-grams}\\
                  \text{60 Most Frequent Special Char 2 and 3-grams}\\
                  \text{20 Most Frequent Pos Tag 2 and 3-grams}\\
                  \text{20 Most Frequent Word 2 and 3-grams}\\
                  \text{150 Highest word frequencies}
                \end{array}
              \right\rbrace = 0.55
$$

In the future, we well need to do a more extensive systematic search for the ideal forest and feature configuration. This somewhat random selection of parameters, wont accurately depict the efficiency of the \nameref{subsec:Generalising Random Forest} approach as a whole.

\subsection{Extended Delta}
% TODO write.

\subsection{Author Specific SVM} \label{subsec:author_specific_svm}
In this approach we work with a mixture of different features. We use the 500
most frequent 3-, 4- and 5-character-grams, the 100 most frequent 3- an
4-word-grams, the 20 most frequent 2-, 3- and 4-postag-grams. The most frequent
grams is found in the brown text corpus. We run only on the PAN 2013 dataset
since 2015 contains only a single known text per author and it would therefore
not make sense to use this method. We use the \textit{sklearn} implementation of
\gls{SVM}'s which internally use \textit{libsvm}.

For each author $a$ in the 2013 dataset we compute features for all known texts
$k$ and give them the class 1. Then we choose the $|k|$ number of random texts
written by other authors and give them the class 0. We then train an \gls{SVM}
classifier on the features and classes generated and use it to predict either 0
or 1 for the unknown text. The \gls{SVM} use a rbf kernel with $C=1$ and
$\gamma = frac{1}{(2|k|)}$. Using that approach we get an average accuracy over
100 runs of 0.736.

\subsection{Author Specific Random Forest}
In this approach we use the same features as in
\ref{subsec:author_specific_svm}. The only difference is that we train a random
forest instead of an \gls{SVM} on the data. The forest has 7 decision trees and
we use the \textit{sklearn} implementation. Using that approach we get an
accurate accuracy over 100 runs of 0.607.
