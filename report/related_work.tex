% TODO: Report both AUC and c@1 instead of just the product.
\section{Related Work} \label{sec:related_work}
\cite{stamatos2009} gives a really good overview of the current state and
history of authorship verification and authorship attribution methods.

PAN \footnote{\url{http://pan.webis.de/}} keeps a collection of shared tasks in
digital text forensics. In 2013, 2014 and 2015 the tasks focused on authorship
verification. In this report we work with data from PAN 2013 and PAN 2015. In
the 2013 task a dataset of authors were given. Each author had a collection
of known texts and a single unknown text. The task was then to determine
which of the unknown texts were written by the same author as the known texts
it was grouped with. In the 2015 task a dataset of authors were also given.
However each author had only a single known texts and a single unknown text
and the task was to determine which of the unknown texts belonged to the same
author as the known texts. The PAN 2013 task were ranked by the F1 measure
and the PAN 2015 task were ranked according to the \gls{AUROC} and the c@1
measure \cite{penas2011}. We describe the measures in more detail in Section
\ref{sec:results}. In this section we describe a lot of the approaches used to
solve the PAN 2015 task. When we report a "performance" we mean the product
of the two measures for the PAN 2015 task as that was the final measure they
were ranked by. The results of the PAN 2013, and PAN 2015 competitions on the
English texts can be seen in sections \ref{sec:appendix:pan_2013_results}, and
\ref{sec:appendix:pan_2015_results} respectively.

\cite{juanpablo2015} chose to perform their analysis on the syntactic level.
This was done using syntactic n-grams, which was extracted using a syntactic
analyser designed for the designated language. This approach served to subvert
the topic of text, as the focus would be on the syntactic class of the n-grams
extracted, rather than the word itself. After extracting the n-grams wanted,
filtering was performed, removing the less frequent n-grams. At this point they
chose to represent their n-grams frequencies as a vector, allowing them to use
the Jaccard distance to measure the difference between new introduced unknown
texts, and their proposed authors' known texts. When the similarity fell under
a certain threshold, the author was deemed non-valid. This yielded a somewhat
mediocre performance of 0.39999 on the English texts, and the authors themselves
noted that a new heuristic handling ill-constructed sentences would probably
have improved their results, as they were just discarded in their case, which
caused very low accuracy in some cases. Additionally, more features describing
other linguistic layers, such as lexical, and syntactic features would probably
improve results as well.

\cite{maitra2015} implemented a solution for the PAN 2015 task. They used a
collection of different features extracted from the text. The features were
based on punctuation, sentence length, vocabulary, character n-grams and
\gls{POS} tagging. They trained a random forest classifier on the features
extracted and used that to determine whether or not the unknown texts were
written by the author. The performance of the method were 0.34749 on English
texts, which is not overwhelming and they commented that deep learning might
make their results better.

\cite{pacheco2015} also proposed using a random forest for the PAN 2015 task.
They implemented two baseline models and one real model. The baseline models
were a simple distance metric with a trained cutoff point and a Gaussian Mixture
Model. The second baseline model was about defining a general feature vector for
all authors and a feature vector for each specific author, called a \gls{UBM}.
To determine if a text from an unknown author was written by a given author,
you compute the distance between the texts feature set and the universal and
author specific feature set. If the unknown text is closer to universal than to
the author specific it is presumed to not be written by the author. The used
features in this case were number of stop words, number of sentences, number
of paragraphs, spacing, punctuation, word frequencies, character frequencies,
punctuation frequencies, lexical density, word diversity, unique words and
unique words over all authors. The main model made use a random forest and a
\gls{UBM} again. A feature vector was yet again computed for all the known
author-known texts in the dataset, the \gls{UBM}. I addition to that, a feature
vector was computed on each individual known text of the dataset. Each of these
author specific feature vectors were then encoded using the \gls{UBM}. The
encoded vectors were then combined, with and author-specific and non-specific
text after which the result were fed to a random forest model. Their performance
were 0.43811 on English texts.

\cite{bartoli2015b} proposed yet another random forest based approach.
They didn't use a random forest classifier as \cite{maitra2015} and
\cite{pacheco2015} but a random forest regressor. The used features were word
n-grams, character n-grams, \gls{POS} tag n-grams, word lengths, sentence
lengths, sentence length n-grams, word richness \footnote{Word richness is
number of distinct words in a text divided by the total number of words.},
punctuation n-grams and text shape n-grams. They then performed a feature
selection and normalization. They performed the final regression with both
trees, a random forest and an SVM. They ended up choosing the random forest as
it performed the best. Their results were very good having the best performance
on Spanish texts. However their English performance was only 0.323.

\cite{castro2015Paper} present an approach that focuses more on the feature
extraction, than the algorithm applied to it. Using a set of 10 features
spanning across 3 different linguistic layers, the character layer, the lexical
layer and the semantic layer. For each of these 10 features a vote is cast. This
vote is determined by comparing the average similarity of the authors' texts.
In the scenario where a new unknown document is introduced, and the validity of
the author needs to be determined, the following happened: For each author, the
similarity of their texts are computed using one of the 10 feature vectors as
input for a chosen similarity function. The similarity for each of these authors
are then averaged to form the \gls{AGS}. The new document is then added to the
group of documents of the proposed author, and the similarity of that group of
documents is computed. If that similarity is above \gls{AGS}, then the author
will be considered valid, and a vote for this decision is thrown. However,
before the vote for the feature is finalized, this is done with 3 different
similarity functions, Cosine, Dice and 1-MinMax. The majority vote of these
3, similarity functions, determine what the vote should be on that specific
feature. This is done for each of the 10 features, where in case of a tie vote
(5 against 5) no decision is taken. This yielded some very good results, however
some questions were raised as to the accuracy of documents on other genres. The
performance of this average based voting method, ended up being the second best
in the PAN 2015 competition with a score of 0.52041 on English texts.

\cite{gutierrez2015} use a somewhat different approach by using Homotopy-based
Classification in their work. Their approach uses a set of 4 features. Bag
of words, Bigram of words, Punctuation and trigram of words. All represent
frequencies. From there a set of $N$ imposters are created, using the generic
imposter method. The \textit{L}-1 homotopy is then applied, constructing a
feature set. That feature set will be matching a document generated using the
imposters and a known text. The unknown document and the reconstructed document
are then compared using what is called the computed residual. This residual
is compared to each author in the set, and if it doesn't match the proposed
author, the author isn't considered the writer of the unknown text. The method
performed well for all the languages used in their test besides
Dutch which they explain is because of the short texts provided. The performance
was 0.51.

\cite{gomezadorno2015} tried to solve the PAN 2015 task by using a graph based
approach. The graph used is an \gls{ISG} which represents the text by creating
a graph for each sentence and combining those graphs into one large graph.
The authors constructed such a graph for each text and used commonalities in
shortest paths in the graph to compare the texts. The results were not very
good, relative to the other entries in the competition, with a performance of
0.2809.

\cite{layton:2014} makes use of a more simple approach. He makes use of
\gls{LNG}. This is used to compute a feature vector for each document written
by a specific author. Collectively they can be combined to a matrix describing
the writing style of that author. When a new unknown text is introduced, one of
three comparative algorithms is used to compute the average similarity between
each of the authors' known texts (Intra-Distance). The average similarity between
the unknown text and the known ones by the proposed author is then computed as
well (Inter-Distance). The author is considered correct if the Inter-Distance
is lower than the Intra-Distance plus 2 times the known datasets' standard
deviation. This was performed using the \gls{CNG}, \gls{SCAP} and \gls{RLP} as
the methods for comparing the different texts. It did however, not perform very
well having a final score of 0.36277.

\cite{castro2015} solved the PAN 2014 task by using the average similarity of
an unknown text to known texts of an author. The features used were character
n-grams, character n gram prefixes, character n gram postfixes, word n grams,
punctuation, \gls{POS} tagging n grams, \gls{POS} tagging at start of sentences
and \gls{POS} tagging at the end of sentences. \cite{castro2015} tried several
different similarity measures Cosine, Dice, Jaccard, Tanimoto, Euclidean and
MinMax. They generally got the best results with Dice and Jaccard similarity.
The results obtained were quite good, as they placed number two in the PAN 2015
competition with a final score of 0.52041.

\cite{hansen2014} and \cite{aalykke2016} both describe usage of authorship
attribution methods in identifying authors of texts written in Danish secondary
schools. \cite{aalykke2016} mainly used a distance based approach. They
extracted features and then used different distance metrics to compute the
closest and therefore best author. \cite{hansen2014} used \gls{SVM}s for the
author classification. They obtained an accuracy of 84\% .
