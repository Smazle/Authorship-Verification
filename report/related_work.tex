\section{Related Work} \label{sec:related_work}
\cite{stamatos2009} gives a really good overview of the current state and
history of authorship verification and authorship attribution methods.

PAN \footnote{\url{http://pan.webis.de/}} keeps a collection of shared tasks in
digital text forensics. In 2013, 2014 and 2015 the tasks focused on authorship
verification among other things. In this report we work with data from PAN 2013
and PAN 2015. In the 2013 task a dataset of authors were given. Each author
had a collection of known texts and a single unknown text. The task was to
determine which of the unknown texts was written by the same author as the
known texts it was grouped with. In the 2015 task a dataset of authors was also
given. However, each author had only a single known text and a single unknown
text, and the task was to determine which of the unknown texts belonged to
the same author as the known texts. The PAN 2013 task was ranked by the F1
measure and the PAN 2015 task was ranked according to the \gls{AUROC} and
the c@1 measure \cite{penas2011}. We describe the measures in more detail in
Section \ref{sec:results}. In this section we describe many approaches used to
solve the PAN 2015 task. When we report a "final score" we mean the product
of the two measures for the PAN 2015 task as that was the final measure they
were ranked by. The results of the PAN 2013, and PAN 2015 competitions on the
English texts are found Appendix \ref{sec:appendix:pan_2013_results}, and
\ref{sec:appendix:pan_2015_results} respectively.

\cite{juanpablo2015} chose to perform their feature extraction on the syntactic
level. This was done using syntactic n-grams, which was extracted using a
syntactic analyser designed for the designated language. After extracting the
n-grams wanted, filtering was performed, removing the less frequent n-grams.
At this point they chose to represent their n-gram frequencies as a vector,
allowing them to use the Jaccard distance to measure the difference between new
introduced unknown texts, and their proposed authors' known texts. When the
similarity fell under a certain threshold, the author was deemed non-valid. This
yielded a final score of 0.39999 on the English texts, and \cite{juanpablo2015}
noted that a new heuristic handling ill-constructed sentences would probably
have improved their results, as they were just discarded in their case.
Additionally, more features describing other linguistic layers, such as lexical,
and syntactic features would probably improve their results as well.

\cite{maitra2015} implemented a solution for the PAN 2015 task. They used a
collection of different features extracted from the known texts. The features
were based on punctuation, sentence length, vocabulary, character n-grams and
\gls{POS} tagging. They trained a Random Forest Classifier on the features
extracted and used that to determine whether or not the unknown texts were
written by the author. The final score of the method was 0.34749 on English
texts, which is not overwhelming and they commented that deep learning might
make their results better.

\cite{pacheco2015} also proposed using a random forest for the PAN 2015 task.
They implemented two baseline models and one real model. The baseline models
were a simple distance metric with a trained cutoff point and a Gaussian Mixture
Model. The second baseline model was about defining a general feature vector for
all authors and a feature vector for each specific author.
To determine if a text from an unknown author is written by a given author,
you compute the distance between the texts feature set and the universal and
author specific feature set. If the unknown text is closer to the universal
text than to the author specific text it is presumed to not be written by
the author. The used features in this case were number of stop words, number
of sentences, number of paragraphs, spacing, punctuation, word frequencies,
character frequencies, punctuation frequencies, lexical density, word diversity,
unique words and unique words over all authors. The main model made use of a
random forest and a \gls{UBM}. A feature vector was again computed for all the
known texts in the dataset and used to construct the \gls{UBM}. I addition
to that, a feature vector was computed on each individual known text of the
dataset. Each of these author specific feature vectors were then encoded using
the \gls{UBM}. The encoded vectors were then combined, with a known and unknown
text after which the result were fed to a Random Forest model. Their final score
was 0.43811 on English texts.

\cite{bartoli2015b} proposed yet another Random Forest based approach.
They didn't use a Random Forest Classifier as \cite{maitra2015} and
\cite{pacheco2015} but a random forest regressor. The used features were word
n-grams, character n-grams, \gls{POS} tag n-grams, word lengths, sentence
lengths, sentence length n-grams, word richness \footnote{Word richness is
number of distinct words in a text divided by the total number of words.},
punctuation n-grams and text shape n-grams. They then performed a feature
selection and normalization. They performed the final regression with both
trees, a Random Forest and a \gls{SVM}. They ended up choosing the Random
Forest as it performed the best. Their results were very good, having the best
final score on Spanish texts. However their English final score was only 0.323.

\cite{castro2015Paper} present an approach that focuses more on the feature
extraction, than the algorithm applied to it. They made use of a set of 10
features spanning across 3 different linguistic layers, the character layer,
the lexical layer and the semantic layer. For each of these 10 features a vote
is cast. The vote is determined by comparing the average similarity of the
authors' texts. When classifying an unknown text as either written by the same
author or another author the following steps are performed: For each author,
the similarity of their texts are computed using one of the 10 feature vectors
as input for a chosen similarity function. The similarity for each of these
authors are then averaged to form the \gls{AGS}. The new document is then added
to the group of documents of the proposed author, and the similarity of that new
group of documents is computed. If that similarity is above the \gls{AGS}, the
unknown text will be classified as being written by the author, and a vote for
this decision is thrown. However, before the vote for the feature is finalized,
it is done with 3 different similarity functions, Cosine, Dice and 1-MinMax. The
majority vote of these 3, similarity functions, determine what the vote should
be on that specific feature. This is done for each of the 10 features, where in
case of a tie vote (5 against 5) no decision is taken. This yielded some very
good results, however some questions were raised as to the accuracy of documents
on other genres. The final score of this average based voting method, ended up
being the second best in the PAN 2015 competition with a score of 0.52041 on
English texts.

\cite{gutierrez2015} use a somewhat different approach by using Homotopy-based
Classification in their work. Their approach use a 4 different features. Bag
of words, bigram of words, Punctuation and trigram of words. From there a set
of imposters are created, using the generic imposter method. The \textit{L}-1
homotopy is then applied, constructing a feature set. That feature set will be
matching a document generated using the imposters and a known text. The unknown
document and the reconstructed document are then compared using what is called
the computed residual. This residual is compared to each author in the set,
and if it doesn't match the proposed author, the author isn't considered the
writer of the unknown text. The method performed well for all the languages used
in their test besides Dutch which they explain is because of the short texts
provided. The final score was 0.51.

\cite{gomezadorno2015} solved the PAN 2015 task by using a graph based approach.
The graph used is an \gls{ISG} which represents the text by creating a graph
for each sentence and combining those graphs into one large graph. The authors
constructed such a graph for each text and used commonalities in shortest paths
in the graph to compare the texts. The results were not very good, relative to
the other entries in the competition, with a final score of 0.2809.

% TODO: Describe what LNG (Local n grams) are!
% TODO: Describe what CNG (Common n grams) are!

\cite{layton:2014} makes use of a more simple approach. He makes use of
different types of N-Grams. These N-Grams was used to compute a feature
vector for each document written by a specific author. Collectively they can
be combined to a matrix describing the writing style of that author. When a
new unknown text is introduced, one of three comparative algorithms is used
to compute the average similarity between each of the authors' known texts
(Intra-Distance). The average similarity between the unknown text and the
known ones by the proposed author is then computed as well (Inter-Distance).
The author is considered correct if the Inter-Distance is lower than the
Intra-Distance plus 2 times the known datasets' standard deviation. This was
performed using different comparative metrics. It did however, not perform very
well having a final score of 0.36277.

\cite{castro2015} solved the PAN 2014 task by using the average similarity of
an unknown text to known texts of an author. The features used were character
n-grams, character n gram prefixes, character n gram postfixes, word n grams,
punctuation, \gls{POS} tagging n grams, \gls{POS} tagging at start of sentences
and \gls{POS} tagging at the end of sentences. \cite{castro2015} tried several
different similarity measures Cosine, Dice, Jaccard, Tanimoto, Euclidean and
MinMax. They generally got the best results with Dice and Jaccard similarity.
Their approach was only applied to spanish texts in this case, and as such
the results aren't relevant for comparison in our case.

\cite{hansen2014} and \cite{aalykke2016} both describe usage of authorship
attribution methods in identifying authors of texts written in Danish secondary
schools. \cite{aalykke2016} mainly used a distance based approach. They
extracted features and then used different distance metrics to compute the
closest and therefore best author. \cite{hansen2014} used \gls{SVM}s for the
author classification. They obtained an accuracy of 84\% .
