\section{Discussion}
Throughout the making of this paper, several oddities occurred, which
needs addressing in order to better understand the behavior of our implemented
verification models. 
In in the real world, authorship-verification problems 
like this are especially relevant for companies like MaCom, who
operates the website, used for submission of high-school students assignments
throughout Denmark. 
While both the PAN2013, and PAN2015 task aims to implement 
authorship-verification, they both represent extremes in that very area. 
PAN2013, lends itself very well to author specific methods, as it contains
a lot of text for each author, but a small amount of authors, and PAN2013
lends itself more the generalized approach due to the small amount of text
per author.
As such relating our method to the real world example of MaCom might give some 
perspective as well. The data MaCom has, closely resembles the data from the
PAN 2013 task. However, with a lot more authors and no unknown text.


Starting off with the Generalized Random Forest 
\ref{subsec:Generalising Random Forest}. This method was off to a bad
start, as the paper it was based on \cite{pacheco2015} only manged to
get a 6'th place in the PAN2015 authorship verification task, and out 
implementation of that same method only ranked 9'th.
\ref{subsec: Pan2015Res}
The main focus of this method however, was the way it attempted to circumvent 
the lack of data, associated with each specific author, by instead learning on
the know-text data-set in its' entirety. 
This approach, actually worked as seen in the test results. By learning based on
a \gls{UBM}, we were able to improve the results compared to learning
on the author-specific feature difference.
The lack of text associated with each author also left quite an impact in
terms of what features were relevant. After training our random forest
model, it became apparent that there was no case where word N-grams performed
well, in terms of the classification. The suspected reason for this, might
be that very lack of text. The texts being limited to about 150 words, 
means that there are only that many word N-grams in the text, which limit
the variety of word N-grams. However, the word-N-grams extracted from the brown
corpus, varies a lot, and as such only a few of the word-N-grams extracted from
the brown corpus are in the actual text, this only becomes more evident as N
increases due to the limited size of the texts. 
As such, while the feature might very will have a high impact in some
cases, we suspect the impact will be averaged out over all the trees in the 
random forest.

If this \gls{UBM} method was to be applied to the MaCom data set, time complexity would
also have to analyzed in order to determine the viability of the Generalized RF 
method.
By only considering $\log(N)$ features in our case, 
where N is the total number of features, we are able to handle large amount of data. 
The time complexity of the method can be broken down into two parts.
The creating of a tree, and number of features considered.
The time complexity of a single decision tree, is $O(n \log{n})$, where n is the
total number of leafs on the tree. In our case,
each tree considers $\log{N}$ features. This gives us a final time 
complexity of $O(\log{N} \cdot n \log{n})$.\cite{RFTime}

While the method didn't provide any impressive results, the generalized approach
might very well be useful the future, in case one comes across another data
sparse set. An improvement that could be done however, was the increase of 
features fed to the random forest. Since it by its' very nature
selects the best features for classification, we can at the cost of 
some run time, train on a much larger feature-set and we suspect
some better results might be produced.


\begin{itemize}
    \item Why did the delta method perform so much better on the training data?
    \item Comparison of our test results to the best ones in PAN 2013 and PAN
        2015.
    \item How does different methods scale to huge amounts of data?
    \item Discuss potential usage in future project based on TPR and TNR.
    \item What improvements could be made to any of the method used. RF Done
    \item Why did SVM's perform so well (https://link.springer.com/article/10.1023/A:1023824908771).
        \begin{itemize}
            \item SVM's can handle many thousands of features.
        \end{itemize}
    \item Manhattan performs better than euclidean in extended delta.
\end{itemize}
