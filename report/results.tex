\section{Results}
As described earlier the PAN2013 results are ranked using the F1 measure and the
PAN 2015 results are ranked using the c@1 and AUROC measures. In our case both
F1 and c@1 corresponds to an accuracy. That can be seen from the definitions,

As described earlier the PAN 2013 results are ranked using the F1 measure. The
measure is defined using \textit{precision} and \textit{recall} which in this
case is defined as,

\begin{align}
    precision &=  correct\_answers / answers \\
    recall &= correct\_answers / problems
\end{align}

Since we answer all problems $problems$ and $answers$ are the same in our case
and we therefore get the F1 is the same as an accuracy.

\begin{equation}
    F1 = 2 \frac{precision \cdot recall}{precision + recall}
        = 2 \frac{accuracy^2}{2accuracy}
        = \frac{2accuracy^2}{2accuracy}
        = \frac{accuracy^2}{accuracy}
        = accuracy.
\end{equation}

Similarly as described earlier the PAN2015 results are ranked using the product
of AUROC and c@1. Like F1 in PAN 2015 the c@1 also corresponds to an accuracy in
our case since we answer all questions. % TODO Maybe move definition of c@1here.

In this section we will describe how we have tested our solutions on the test
datasets and give the results of those tests.

\subsection{Delta Method}
The Delta method was tested by creating the same features for both the training
dataset and the test datasets. We then computed the mean and standard variance
of the training set and used that to normalize both the training and test
dataset. For each text in the test dataset we then drew differing numbers of
opposing texts from the training dataset. Those opposing texts were used as the
opposition in the Delta Method. The results for running the delta method on the
two test sets included in PAN 2013 and one dataset included in PAN 2015 is shown
in Table \ref{tab:delta_method_final_results}. The AUROC curve for the Delta
Method is shown in Figure \ref{fig:delta_method_roc}. It is created by computing
the \gls{TPR} and \gls{FPR} for the differing number of opposing authors.

\begin{table}
    \centering
    \begin{tabular}{l|lll}
        \textbf{Opponents} & \textbf{PAN 2013 Test 1} & \textbf{PAN 2013 Test 2}
        & \textbf{PAN 2015 Test} \\ \hline
        1  & 0.58685 & 0.58283 & 0.55870 \\
        2  & 0.62235 & 0.60125 & \textbf{0.56550} \\
        3  & 0.63348 & 0.60771 & 0.53969 \\
        4  & 0.63741 & 0.62173 & 0.52880 \\
        5  & 0.63483 & 0.61897 & 0.52269 \\
        6  & 0.63876 & 0.62787 & 0.52080 \\
        7  & 0.64337 & 0.63188 & 0.51399 \\
        8  & 0.64123 & 0.63488 & 0.51270 \\
        9  & \textbf{0.64438} & 0.63173 & 0.51320 \\
        10 & 0.64235 & \textbf{0.63291} & 0.50840
    \end{tabular}
    \caption{Result of running the delta method on two test sets included in PAN
    2013 and single test set included in PAN 2015 with different number of
    authors. The best score obtained on each dataset is highlighted.}
    \label{tab:delta_method_final_results}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{./pictures/delta_method_roc.png}
    \caption{The ROC curve of the delta method with number of opposing authors
    varying from 0 to 10 using the two test datasets for PAN2013 and the test
    dataset for PAN2015.}
    \label{fig:delta_method_roc}
\end{figure}

\subsection{Generalizing Random Forest}
After creting our \gls{UBM} based on the training data and training our random
forest using the two different encodings presented, we got the following
results,

\begin{align}
\text{UBM Enconding Accuracy}:&\qquad 0.616\\
\text{Subtraction Enconding Accuracy}:& \qquad 0.604
\end{align}

\subsection{Extended Delta}

\subsection{Author Specific SVM}
The Author Specific SVM is tested by generating features for the training and
test datasets for the PAN 2013 texts. For each different author in the test
dataset we extract features for all their known texts and their single unknown
text. We then draw random texts from the training dataset which will serve as
opponents to the texts written by the author. Then we train an SVM using the
texts known to be written by the author and the texts from the training dataset
and predict the unknown text using that SVM.

The best configuration on the training set were configuration B using the
frequencies of the 300 most frequent words. On the two test datasets it obtained
accuracies of 0.78650 and 0.78000.

%In the training we tried two different sets of features to train the SVM. They
%both performed about as well as the other and we will therefore
%The accuracy for the two test sets of the PAN 2013 was, 0.85050 and 0.84466.
