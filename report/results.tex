\section{Results} \label{sec:results}
As described earlier the PAN 2013 results are ranked using the F1 measure. The
measure is defined using \textit{precision} and \textit{recall} which in PAN
2013 is defined as,

\begin{align}
    precision &=  \frac{correct\_answers}{answers} \\[1em]
    recall &= \frac{correct\_answers}{problems}
\end{align}

Since we answer all problems, $problems$ and $answers$ are the same in our case
and therefore the F1 measure is the same as an accuracy,

\begin{equation}
    F1 = 2 \frac{precision \cdot recall}{precision + recall}
        = 2 \frac{accuracy^2}{2accuracy}
        = \frac{2accuracy^2}{2accuracy}
        = \frac{accuracy^2}{accuracy}
        = accuracy.
\end{equation}

Similarly as described earlier the PAN 2015 results are ranked using the product
of \gls{AUROC} and c@1. The \gls{AUROC} is a measure of discrimination. That
is, it measures the ability of a solution to distinguish between texts written
by the same author and texts written by another author. An \gls{AUROC} score is
generally considered excellent when between 0.9 and 1, good when between 0.8
and 0.9, fair when between 0.7 and 0.8, poor when between 0.6 and 0.7 and a
failure when between 0.5 and 0.6. The c@1 measure is chosen since it measures
performance in the binary case. The c@1 measure does not use probabilities but
classifies everything above 0.5 as a yes everything below as a no and a 0.5 as a
don't know. Like F1 in PAN 2015, the c@1 also corresponds to an accuracy in our
case since we answer all questions. The definition of c@1 is,

\begin{equation}
    c@1 = \frac{1}{n} \left(n_c + \frac{n_u \cdot n_c}{n}\right)
\end{equation}

where $n$ is the number of problems, $n_c$ is the number of correct answers and
$n_u$ is the number of unanswered problems. So when $n_u$ is 0 we have,

\begin{equation}
    c@1 = \frac{1}{n} \left(n_c + \frac{n_u \cdot n_c}{n}\right)
        = \frac{1}{n} \left(n_c + \frac{0 \cdot n_c}{n}\right)
        = \frac{1}{n} n_c
        = accuracy.
\end{equation}

In this section we will describe how we have tested our solutions on the test
datasets and give the results of those tests. When we are testing on the PAN
2013 dataset we will report an accuracy as that is the only performance measure.
When we are testing on the PAN 2015 dataset we will report both the \gls{AUROC}
and the accuracy since both are used to measure performance.

\subsection{Delta Method} \label{subsec:results:delta_method}

The Delta method was tested by creating the same features for both the training
dataset and the test datasets. We then computed the mean and standard variance
of the training set and used that to normalize both the training and test
dataset. For each text in the test dataset we then drew differing numbers of
opposing texts from the training dataset. Those opposing texts were used as
the opposition in the Delta Method. The number of opposing authors we used
were the ones we found in the training section and were 4 for PAN 2013 and 1
for PAN 2015. The results for running the Delta Method on the two test sets
included in PAN 2013 and one test set included in PAN 2015 is shown in Table
\ref{tab:delta_method_final_results}. The \gls{ROC} curve for the Delta Method
is shown in Figure \ref{fig:delta_method_roc}. It is created by computing the
\gls{TPR} and \gls{FPR} for differing number of opposing authors.

\begin{table}
    \centering
    \begin{tabular}{c|ccc}
        & \textbf{PAN 2013 Dataset 1} & \textbf{PAN 2013 Dataset 2} & \textbf{PAN 2015 Dataset}\\
        \hline
        \textbf{Accuracy}  & 0.63191 & 0.61314 & 0.57850 \\
        \textbf{\gls{TPR}} & 0.51900 & 0.52492 & 0.54899 \\
        \textbf{\gls{TNR}} & 0.72408 & 0.70000 & 0.60800
    \end{tabular}
    \caption{Result of running the delta method on two test sets included in PAN
    2013 and single test set included in PAN 2015 with 4 opposing authors for
    the PAN 2013 set and 1 opposing author for PAN 2015.}
    \label{tab:delta_method_final_results}
\end{table}

The \gls{AUROC} for the Delta Method on the PAN 2015 data was 0.59121. Resulting
in a final score of $0.57850 \cdot 0.59121 = 0.34201$ for the PAN 2015 set.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{./pictures/delta_method_roc.png}
    \caption{The ROC curve of the delta method with number of opposing authors
    varying from 0 to 100 using the test dataset for PAN 2015.}
    \label{fig:delta_method_roc}
\end{figure}

\subsection{Generalising Random Forest} \label{subsec:results:generalising_random_forest}
In order to test the \gls{UBM} approach proposed by \cite{pacheco2015}, we start
by computing our feature set and generating our \gls{UBM} as described in
Section \ref{subsec:method:generalising_random_forest}.

The feature set and \gls{UBM}, are then encoded according to Equation
\eqref{eq:rf-encode}, and are used to train a random forest with parameters
matching the ones used in experiments. That is all parameters being set to their
default except \texttt{n\_estimators} that is set to 1000. At this point we
encode the feature set created from the test data, against the \gls{UBM}, which
is then fed to our trained random forest to get the predictions. This resulted
in an accuracy of 0.60400, \gls{TPR} of 0.63200 and \gls{TNR} of 0.57600 on the
pan 2015 dataset. On the other hand, we also chose to train our model using the
alternate subtraction encoding, which doesn't make use of the \gls{UBM}. The
subtraction encoding under-performed relative to the \gls{UBM} approach, with an
accuracy of 0.58400 a \gls{TPR} of 0.66800 and a \gls{TNR} of 0.50000.

We have generated the \gls{ROC} curve for both Random Forest tests. The
\gls{AUROC} was 0.63868 for the \gls{UBM} method and 0.56821 for the Minus
method. The curves is shown in Figure \ref{fig:forest_roc}. This means that the
Final Scores, of the methods are:

\begin{align}
    \text{Final Score \gls{UBM}} &= c@1 \cdot AUROC = 0.604 \cdot 0.63868 = 0.3858  \\
    \text{Final Score Minus} &= c@1 \cdot AUROC = 0.584 \cdot 0.56821 = 0.3318
\end{align}

The \gls{ROC} curve was generated by having the Random Forest give the
probabilities for each unknown text belonging to the same author as the known.
We then chose different thresholds between 0 and 1 (specifically $\{0.0, 0.01,
\dots, 1.0\}$) and computed the \gls{TPR} and \gls{FPR} for each of them. We
then plotted those results as a function from \gls{FPR} to \gls{TPR}.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{./pictures/forest_roc.png}
    \caption{The ROC curve of the two Generalising Random Forest
    approaches.}
    \label{fig:forest_roc}
\end{figure}

\subsection{Extended Delta} \label{subsec:results:extended_delta}

The Extended Delta Method is tested by generating features according to the best
configurations found in the training phase. Then the same procedure used to
test the regular delta method is employed. The best configuration for the PAN
2013 data were the 100 most frequent character-2, -3 and 4-grams and the 300
most frequent words for 5 opposing authors. The best configuration for the PAN
2015 data were the 20 most frequent special-character-1, -2 and 3-grams for 2
opposing authors. The accuracies, \gls{TPR}s and \gls{TNR}s obtained on all test
datasets are shown in Table \ref{tab:extended_delta_method_final_results}. The
\gls{ROC} curve is shown in Figure \ref{fig:extended_delta_method_roc} and the
\gls{AUROC} was 0.65188.

\begin{table}
    \centering
    \begin{tabular}{c|ccc}
        & \textbf{PAN 2013 Dataset 1} & \textbf{PAN 2013 Dataset 2} & \textbf{PAN 2015 Dataset} \\
        \hline
        \textbf{Accuracy}  & 0.72528 & 0.67291 & 0.61949 \\
        \textbf{\gls{TPR}} & 0.66750 & 0.75761 & 0.49760 \\
        \textbf{\gls{TNR}} & 0.77244 & 0.58953 & 0.74140
    \end{tabular}
    \caption{Result of running the extended delta method on two test sets
    included in PAN 2013 and single test set included in PAN 2015 with the best
    configurations found in the training phase.}
    \label{tab:extended_delta_method_final_results}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{./pictures/extended_delta_method_roc.png}
    \caption{The ROC curve of the Extended Delta Method with number of opposing
    authors varying from 0 to 100 using the test dataset for for PAN 2015.}
    \label{fig:extended_delta_method_roc}
\end{figure}

\subsection{Author Specific SVM} \label{subsec:results:author_specific_svm}

The Author Specific SVM is tested by generating features for the training and
test datasets on the PAN 2013 texts. For each author in the test dataset we
extract features for all their known texts and their single unknown text. We
then draw random texts from the training dataset which will serve as opponents
to the texts written by the author. Then we train an SVM using the texts known
to be written by the author and the texts from the training dataset and predict
the unknown text using that SVM. The hyperparameters for the SVM is the ones we
found best on the training dataset.

The best configuration on the training set was configuration B using the
frequencies of the 300 most frequent words. The results on the two test datasets
are shown in Table \ref{table:svm_results}.

\begin{table}
    \centering
    \begin{tabular}{c|cc}
        & \textbf{Test Dataset 1} & \textbf{Test Dataset 2} \\
        \hline
        \textbf{Accuracy}  & 0.77650 & 0.78066 \\
        \textbf{\gls{TPR}} & 0.71444 & 0.70785 \\
        \textbf{\gls{TNR}} & 0.82727 & 0.84437
    \end{tabular}
    \caption{Results of the Author Specific SVM on the two test datasets for PAN
    2013.}
    \label{table:svm_results}
\end{table}
